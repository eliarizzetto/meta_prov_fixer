#!/usr/bin/env python3
"""
Script to apply SPARQL updates to database for already detected issues.

This script reads issues from JSON-Lines files generated by dry-run mode
and executes corresponding SPARQL UPDATE queries to database.
"""

import os
import json
import time
import argparse
import logging
from pathlib import Path
from datetime import datetime, date
from typing import List, Tuple
from rdflib import URIRef, Literal
from rdflib.namespace import XSD
import traceback

from sparqlite import SPARQLClient, QueryError, EndpointError
from meta_prov_fixer.src import (
    FillerFixerFile,
    DateTimeFixerFile,
    MissingPrimSourceFixerFile,
    MultiPAFixerFile,
    MultiObjectFixerFile,
    sparql_update
)
from meta_prov_fixer.utils import batched, normalise_datetime, get_previous_meta_dump_uri
from meta_prov_fixer.virtuoso_watchdog import start_watchdog_thread


class SparqlUpdatesCheckpoint:
    """
    Checkpoint class for tracking progress in update_db_from_issues.py.
    
    Tracks which JSON-Lines files have been processed and how many issues
    of each type have been applied to database.
    """
    
    def __init__(self, path: str):
        self.path = path
        self.state = None
        self.dirty = False
        self.load()

    def load(self):
        """Load checkpoint state from file if it exists."""
        if os.path.exists(self.path):
            with open(self.path, "r", encoding="utf-8") as f:
                self.state = json.load(f)
        else:
            self.state = None

    def _atomic_write(self, data: dict, retries: int = 5, delay: float = 0.05):
        """Atomically write data to checkpoint file."""
        tmp_path = self.path + ".tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        for i in range(retries):
            try:
                os.replace(tmp_path, self.path)
                return
            except PermissionError:
                time.sleep(delay)
        raise PermissionError(f"Failed to replace checkpoint file after {retries} retries")

    def update_state(
        self,
        current_file: str,
        line_number: int,
        ff_applied: int,
        dt_applied: int,
        mps_applied: int,
        pa_applied: int,
        mo_applied: int
    ):
        """Update checkpoint state with current progress."""
        self.state = {
            "current_file": current_file,
            "line_number": line_number,
            "ff_applied": ff_applied,
            "dt_applied": dt_applied,
            "mps_applied": mps_applied,
            "pa_applied": pa_applied,
            "mo_applied": mo_applied,
            "timestamp": datetime.now().isoformat()
        }
        self.dirty = True

    def flush(self):
        """Flush checkpoint state to disk."""
        if self.dirty and self.state:
            self._atomic_write(self.state)
            self.dirty = False

    def get_resume_line(self, filename: str) -> int:
        """
        Get line number to resume from for a given file.
        
        Returns 0 if no checkpoint or different file.
        Returns checkpointed line number if resuming same file.
        """
        if not self.state or self.state["current_file"] != filename:
            return 0
        return self.state["line_number"]

    def get_applied_counts(self) -> Tuple[int, int, int, int, int]:
        """Get counts of applied issues from checkpoint."""
        if not self.state:
            return (0, 0, 0, 0, 0)
        return (
            self.state["ff_applied"],
            self.state["dt_applied"],
            self.state["mps_applied"],
            self.state["pa_applied"],
            self.state["mo_applied"]
        )


def stream_and_fix_on_db(
    issues_dir: str,
    endpoint: str,
    meta_dumps: List[Tuple[str, str]],
    failed_log_fp: str,
    chunk_size: int = 100,
    checkpoint: SparqlUpdatesCheckpoint = SparqlUpdatesCheckpoint("sparql_update_db.checkpoint.json"),
    resume: bool = True
):
    """
    Stream JSON-Lines files and apply SPARQL updates in batches to the database.
    
    Args:
        issues_dir: Directory containing JSON-Lines files storing detected issues
        endpoint: SPARQL endpoint URL for executing queries
        meta_dumps: Meta dumps register
        failed_log_fp: Path to failed queries log
        chunk_size: Number of issues per SPARQL update batch
        checkpoint: Checkpoint object for resuming
        resume: If True, use checkpoint to skip already-processed data
    """
    client = SPARQLClient(endpoint)
    issues_dir = Path(issues_dir)
    jsonl_files = sorted(issues_dir.glob('*.jsonl'))
    logging.info(f"Found {len(jsonl_files)} JSON-Lines file(s)")
    logging.info(f"Checkpoint state at start: {checkpoint.state}")
    
    # Counters for statistics
    total_files = 0
    total_ff_issues = 0
    total_dt_issues = 0
    total_mps_issues = 0
    total_pa_issues = 0
    total_mo_issues = 0
    
    # Load applied counts from checkpoint if resuming
    if resume and checkpoint.state:
        total_ff_issues, total_dt_issues, total_mps_issues, total_pa_issues, total_mo_issues = checkpoint.get_applied_counts()
        logging.info("Resuming from checkpoint:")
        logging.info(f"  Filler issues already applied: {total_ff_issues}")
        logging.info(f"  DateTime issues already applied: {total_dt_issues}")
        logging.info(f"  Missing PS issues already applied: {total_mps_issues}")
        logging.info(f"  Multi PA issues already applied: {total_pa_issues}")
        logging.info(f"  Multi Object issues already applied: {total_mo_issues}")
    
    # Batch accumulators for each issue type
    ff_batch = []
    dt_batch = []
    mps_batch = []
    pa_batch = []
    mo_batch = []
    
    def flush_batches():
        """Flush all issue batches by applying the approriate fixes to the database."""
        nonlocal total_ff_issues, total_dt_issues, total_mps_issues, total_pa_issues, total_mo_issues
        
        if ff_batch:
            apply_filler_issues(client, ff_batch, failed_log_fp, chunk_size)
            total_ff_issues += len(ff_batch)
            ff_batch.clear()
        
        if dt_batch:
            apply_datetime_issues(client, dt_batch, failed_log_fp, chunk_size)
            total_dt_issues += len(dt_batch)
            dt_batch.clear()
        
        if mps_batch:
            apply_missing_ps_issues(client, mps_batch, meta_dumps, failed_log_fp, chunk_size)
            total_mps_issues += len(mps_batch)
            mps_batch.clear()
        
        if pa_batch:
            apply_multi_pa_issues(client, pa_batch, failed_log_fp, chunk_size)
            total_pa_issues += len(pa_batch)
            pa_batch.clear()
        
        if mo_batch:
            apply_multi_object_issues(client, mo_batch, meta_dumps, failed_log_fp, chunk_size)
            total_mo_issues += len(mo_batch)
            mo_batch.clear()
    
    times_per_file = []

    logging.info(f"Processing JSON-Lines files in {issues_dir}...")

    # Process each file line by line
    for file_idx, jsonl_file in enumerate(jsonl_files):
        # Checkpoint-based file skipping
        resume_line = 0
        if resume and checkpoint.state:
            checkpointed_file = checkpoint.state["current_file"]
            
            # Skip files that come before checkpointed file in sorted order
            if jsonl_file.name < checkpointed_file:
                # logging.debug(f"Skipping already completed: {jsonl_file.name}")
                continue
            # Resume from specific line in checkpointed file
            elif jsonl_file.name == checkpointed_file:
                resume_line = checkpoint.state["line_number"]
                logging.info(f"Resuming from file '{jsonl_file.name}', line {resume_line}")
            # Process all files after checkpointed file normally
            else:
                logging.debug(f"Processing: {jsonl_file.name}")
        else:
            logging.debug(f"Processing: {jsonl_file.name}")
        
        start_time = time.time()
        
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                # Skip lines we've already processed
                if resume and line_num <= resume_line:
                    continue
                
                line = line.strip()
                if not line:
                    continue
                
                try:
                    record = json.loads(line)
                    # Count files only when actually processing (not when skipping lines)
                    if line_num == 1 and resume_line == 0:
                        total_files += 1
                    
                    # Filler issues: each item is [graph_uri, {'to_delete': [...], 'remaining_snapshots': [...]}]
                    ff_items = record.get('ff', [])
                    for item in ff_items:
                        if isinstance(item, list) and len(item) == 2:
                            ff_batch.append((item[0], {'to_delete': item[1]['to_delete'], 'remaining_snapshots': item[1]['remaining_snapshots']}))
                        else:
                            ff_batch.append(item)
                    
                    # DateTime issues: each item is [graph_uri, snapshot_uri, predicate_uri, invalid_datetime]
                    dt_batch.extend([tuple(item) for item in record.get('dt', [])])
                    
                    # Missing Primary Source issues: each item is [snapshot_uri, generation_time]
                    mps_batch.extend([tuple(item) for item in record.get('mps', [])])
                    
                    # Multiple Processing Agent issues: each item is [graph_uri, snapshot_uri]
                    pa_batch.extend([tuple(item) for item in record.get('pa', [])])
                    
                    # Multiple Object issues: each item is [graph_uri, generation_time]
                    mo_batch.extend([tuple(item) for item in record.get('mo', [])])
                    
                    # Flush batches when they reach chunk_size
                    if (len(ff_batch) >= chunk_size or 
                        len(dt_batch) >= chunk_size or 
                        len(mps_batch) >= chunk_size or 
                        len(pa_batch) >= chunk_size or 
                        len(mo_batch) >= chunk_size):

                        flush_batches()

                        checkpoint.update_state(
                            jsonl_file.name,
                            line_num,
                            total_ff_issues,
                            total_dt_issues,
                            total_mps_issues,
                            total_pa_issues,
                            total_mo_issues
                        )
                    
                except (Exception, KeyboardInterrupt) as e:
                    checkpoint.flush()
                    print(traceback.print_exc())
                    if type(e) == KeyboardInterrupt:
                        logging.error("KeyboardInterrupt")
                    else:
                        logging.error(e)
        
        
        # Flush any remaining batches after each file
        flush_batches()

        elapsed_file = time.time() - start_time

        times_per_file.append(elapsed_file)
        # logging.debug(f"Finished processing: {jsonl_file.name} in {elapsed_file:.2f} seconds.")
        if file_idx % 5 == 0:
            avg_time = sum(times_per_file) / len(times_per_file)
            est_remaining = avg_time * (len(jsonl_files) - file_idx - 1)
            logging.info(f"Average time per file with last {len(times_per_file)} files: {avg_time:.2f} seconds. Estimated time remaining: {est_remaining/3600:.2f} hours")
            times_per_file.clear()

            # Recreate SPARQL client periodically to avoid pycurl memory issues
            # logging.debug(f"Recreating SPARQLClient files to clear accumulated pycurl state.")
            client.close()
            client = SPARQLClient(endpoint)


        # Mark this file as completed in checkpoint
        checkpoint.update_state(
            jsonl_file.name,
            line_num,  # Last line number
            total_ff_issues,
            total_dt_issues,
            total_mps_issues,
            total_pa_issues,
            total_mo_issues
        )
        checkpoint.flush()
    
    # Final flush to ensure all batches are applied
    flush_batches()
    
    # Final checkpoint update
    checkpoint.update_state(
        "completed",
        0,
        total_ff_issues,
        total_dt_issues,
        total_mps_issues,
        total_pa_issues,
        total_mo_issues
    )
    checkpoint.flush()
    client.close()
    
    # Log statistics
    logging.info("=" * 80)
    logging.info("Summary of issues applied:")
    logging.info(f"  Total files processed: {total_files}")
    logging.info(f"  Filler issues: {total_ff_issues}")
    logging.info(f"  DateTime issues: {total_dt_issues}")
    logging.info(f"  Missing Primary Source issues: {total_mps_issues}")
    logging.info(f"  Multiple Processing Agent issues: {total_pa_issues}")
    logging.info(f"  Multiple Object issues: {total_mo_issues}")
    logging.info(f"  Total: {total_ff_issues + total_dt_issues + total_mps_issues + total_pa_issues + total_mo_issues}")
    logging.info("=" * 80)


def apply_filler_issues(
    client: SPARQLClient,
    ff_issues: List[Tuple[str, dict]],
    failed_log_fp: str,
    chunk_size: int = 100
):
    """Apply filler fixer issues to the database."""
    if not ff_issues:
        return
    
    logging.debug(f"Applying {len(ff_issues)} filler issue(s)")
    
    for chunk in batched(ff_issues, chunk_size):
        for graph_uri, values in chunk:
            to_delete = [URIRef(u) for u in values['to_delete']]
            remaining = [URIRef(u) for u in values['remaining_snapshots']]
            
            rename_mapping = FillerFixerFile.map_se_names(set(to_delete), set(remaining))
            
            if to_delete:
                delete_query = FillerFixerFile.build_delete_sparql_query((URIRef(graph_uri), values))
                sparql_update(client, delete_query, failed_log_fp)
            
            if any(old != new for old, new in rename_mapping.items()):
                rename_query = FillerFixerFile.build_rename_sparql_query(rename_mapping)
                sparql_update(client, rename_query, failed_log_fp)
            
            if remaining:
                newest_names = list(set(rename_mapping.values()))
                adaptime_query = FillerFixerFile.build_adapt_invaltime_sparql_query(graph_uri, [str(n) for n in newest_names])
                sparql_update(client, adaptime_query, failed_log_fp)


def apply_datetime_issues(
    client: SPARQLClient,
    dt_issues: List[Tuple[str, str, str, str]],
    failed_log_fp: str,
    chunk_size: int = 100
):
    """Apply datetime fixer issues to the database."""
    if not dt_issues:
        return
    
    logging.debug(f"Applying {len(dt_issues)} datetime issue(s)")
    
    issues_tuples = [(URIRef(g), URIRef(s), URIRef(p), dt) for g, s, p, dt in dt_issues]
    
    for chunk in batched(issues_tuples, chunk_size):
        query = DateTimeFixerFile.build_update_query(chunk)
        sparql_update(client, query, failed_log_fp)


def apply_missing_ps_issues(
    client: SPARQLClient,
    mps_issues: List[Tuple[str, str]],
    meta_dumps: List[Tuple[str, str]],
    failed_log_fp: str,
    chunk_size: int = 100
):
    """Apply missing primary source issues to the database."""
    if not mps_issues:
        return
    
    logging.debug(f"Applying {len(mps_issues)} missing primary source issue(s)")
    
    issues_tuples = [(URIRef(s), Literal(t, datatype=XSD.dateTime)) for s, t in mps_issues]
    
    for chunk in batched(issues_tuples, chunk_size):
        query = MissingPrimSourceFixerFile.build_update_query(chunk, meta_dumps)
        sparql_update(client, query, failed_log_fp)


def apply_multi_pa_issues(
    client: SPARQLClient,
    pa_issues: List[Tuple[str, str]],
    failed_log_fp: str,
    chunk_size: int = 100
):
    """Apply multiple processing agent issues to the database."""
    if not pa_issues:
        return
    
    logging.debug(f"Applying {len(pa_issues)} multiple processing agent issue(s)")
    
    issues_tuples = [(URIRef(g), URIRef(s)) for g, s in pa_issues]
    
    for chunk in batched(issues_tuples, chunk_size):
        query = MultiPAFixerFile.build_update_query(chunk)
        sparql_update(client, query, failed_log_fp)


def apply_multi_object_issues(
    client: SPARQLClient,
    mo_issues: List[Tuple[str, str]],
    meta_dumps: List[Tuple[str, str]],
    failed_log_fp: str,
    chunk_size: int = 100
):
    """Apply multiple object issues to the database."""
    if not mo_issues:
        return
    
    logging.debug(f"Applying {len(mo_issues)} multiple object issue(s)")
    
    issues_tuples = [(URIRef(g), Literal(t, datatype=XSD.dateTime)) for g, t in mo_issues]
    
    for chunk in batched(issues_tuples, chunk_size):
        query = MultiObjectFixerFile.build_update_query(chunk, meta_dumps)
        sparql_update(client, query, failed_log_fp)


def main():
    parser = argparse.ArgumentParser(
        description="Apply dry-run issues to database from JSON-Lines files"
    )
    parser.add_argument("-e", "--endpoint", type=str, required=True, help="SPARQL endpoint URL")
    parser.add_argument("-d", "--issues-dir", type=str, required=True, help="Directory containing JSON-Lines files with issues")
    parser.add_argument("-m", "--meta-dumps", type=str, required=True, help="Path to meta dumps register JSON file")
    parser.add_argument("--chunk-size", type=int, default=100, help="Number of issues per SPARQL update batch (default: 100)")
    parser.add_argument("--failed-queries-fp", type=str, default=f"sparql_update_db_failed_queries_{datetime.today().strftime('%Y-%m-%d')}.txt", help="Path to log failed SPARQL update queries")
    parser.add_argument("-l", "--log-fp", type=str, default=f"sparql_update_db_{datetime.today().strftime('%Y-%m-%d')}.log", help="File path for the run log")
    parser.add_argument("-c", "--checkpoint-fp", type=str, default="sparql_update_db.checkpoint.json", help="Path to checkpoint file for resuming (default: sparql_update_db.checkpoint.json)")
    parser.add_argument("--no-resume", action="store_true", help="Do not use checkpoint to resume (start from beginning)")
    parser.add_argument("-r", "--auto-restart-container", action="store_true", help="Enable memory watchdog to auto-restart the Virtuoso Docker container when memory usage is too high.")
    parser.add_argument("-v", "--virtuoso-container", type=str, default=None, help="Name of the Virtuoso Docker container (required when --auto-restart-container is used).")
    
    args = parser.parse_args()

    if args.auto_restart_container:
        if not args.virtuoso_container:
            parser.error(
                "--virtuoso-container is required when using --auto-restart-container"
            )

    if args.auto_restart_container:
        print("Starting single Virtuoso watchdog (launcher-controlled)")
        start_watchdog_thread(args.virtuoso_container, args.endpoint)
    else:
        print("Watchdog disabled. Processes will not be auto-restarted.")
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - [%(funcName)s, %(filename)s:%(lineno)d] - %(message)s",
        filename=args.log_fp
    )
    
    logging.info("=" * 80)
    logging.info("Starting dry-run issues application process")
    logging.info(f"Endpoint: {args.endpoint}")
    logging.info(f"Issues directory: {args.issues_dir}")
    logging.info(f"Meta dumps: {args.meta_dumps}")
    logging.info(f"Chunk size: {args.chunk_size}")
    logging.info(f"Failed queries log: {args.failed_queries_fp}")
    logging.info(f"Checkpoint file: {args.checkpoint_fp}")
    logging.info(f"Resume: {not args.no_resume}")
    logging.info("=" * 80)
    
    # Load meta dumps register
    logging.info(f"Loading meta dumps register from: {args.meta_dumps}")
    with open(args.meta_dumps, 'r', encoding='utf-8') as f:
        meta_dumps_raw = json.load(f)
    
    meta_dumps = sorted(
        [(date.fromisoformat(d), url) for d, url in meta_dumps_raw],
        key=lambda x: x[0]
    )
    
    checkpoint = SparqlUpdatesCheckpoint(args.checkpoint_fp)
    
    try:
        # Stream and apply issues from JSON-Lines files
        stream_and_fix_on_db(
            args.issues_dir,
            args.endpoint,
            meta_dumps,
            args.failed_queries_fp,
            args.chunk_size,
            checkpoint,
            not args.no_resume
        )
        
        logging.info("=" * 80)
        logging.info("Successfully applied all SPARQL updates to database!")
        logging.info("=" * 80)
        
    except Exception as e:
        logging.error(f"Error applying issues: {e}")
        logging.error(f"Checkpoint state at error: {checkpoint.state if checkpoint else 'N/A'}")
        raise
    
    finally:
        checkpoint.flush()
    
    # Clean up checkpoint file on successful completion
    if os.path.exists(checkpoint.path):
        os.remove(checkpoint.path)
        logging.info("Checkpoint file cleaned up after successful completion")


if __name__ == '__main__':
    main()